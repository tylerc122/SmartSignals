#!/usr/bin/env python3
"""
Training Script for Traffic Signal Control RL Agent

This script creates and trains a Proximal Policy Optimization (PPO) agent
to control traffic lights at our cross intersection. The agent learns by
interacting with the SUMO simulation thousands of times, getting better
at minimizing traffic congestion through experience.
"""

import os
import sys
import yaml
from datetime import datetime
import numpy as np

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
import torch

from environments.sumo_traffic_env import SumoTrafficEnv


def load_config(config_path="configs/training_config.yaml"):
    """Load training configuration from YAML file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def create_environment(config):
    """
    Create and wrap the SUMO traffic environment.
    
    The Monitor wrapper tracks episode statistics (rewards, lengths)
    The DummyVecEnv wrapper makes it compatible with Stable Baselines3
    """
    # Create the base environment
    env = SumoTrafficEnv(
        sumo_config_file="sumo_scenarios/cross_intersection.sumocfg",
        step_duration=config['environment']['step_size'],
        episode_length=config['environment']['episode_length']
    )
    
    # Wrap with Monitor to track episode statistics
    env = Monitor(env)
    
    # Wrap with DummyVecEnv (required by Stable Baselines3)
    env = DummyVecEnv([lambda: env])
    
    return env


def create_agent(env, config):
    """
    Create the PPO (Proximal Policy Optimization) agent.
    
    PPO is a policy gradient method that's stable and sample efficient.
    It learns a policy (what action to take in each state) and a value function
    (how good each state is) simultaneously.
    """
    # Extract algorithm configuration
    algo_config = config['algorithm']
    network_config = config['network']
    
    # Create PPO agent
    model = PPO(
        policy="MlpPolicy",  # Multi-Layer Perceptron policy network
        env=env,
        learning_rate=algo_config['learning_rate'],
        batch_size=algo_config['batch_size'],
        n_epochs=algo_config['n_epochs'],
        verbose=1,  # Print training progress
        tensorboard_log="./logs/tensorboard/",  # For monitoring training
        device="auto"  # Use GPU if available, otherwise CPU
    )
    
    return model


def setup_callbacks(config):
    """
    Setup training callbacks for monitoring and checkpointing.
    
    Callbacks are functions that run during training to:
    - Save model checkpoints periodically
    - Evaluate performance during training
    - Log metrics for analysis
    """
    # Create directories if they don't exist
    os.makedirs("models/checkpoints", exist_ok=True)
    os.makedirs("logs/eval", exist_ok=True)
    
    # Checkpoint callback - saves model every N steps
    checkpoint_callback = CheckpointCallback(
        save_freq=config['training']['save_frequency'],
        save_path="models/checkpoints/",
        name_prefix="ppo_traffic_model"
    )
    
    return [checkpoint_callback]


def train_agent():
    """Main training function."""
    print("üöÄ Starting Traffic Signal RL Training")
    print("=" * 50)
    
    # Load configuration
    print("üìã Loading configuration...")
    config = load_config()
    print(f"‚úÖ Config loaded: {config['algorithm']['name']} for {config['algorithm']['total_timesteps']} timesteps")
    
    # Create environment
    print("\nüèóÔ∏è  Creating environment...")
    env = create_environment(config)
    print("‚úÖ Environment created and wrapped")
    
    # Create agent
    print("\nü§ñ Creating RL agent...")
    model = create_agent(env, config)
    print(f"‚úÖ {config['algorithm']['name']} agent created")
    print(f"   Learning rate: {config['algorithm']['learning_rate']}")
    print(f"   Batch size: {config['algorithm']['batch_size']}")
    
    # Setup callbacks
    print("\n‚öôÔ∏è  Setting up training callbacks...")
    callbacks = setup_callbacks(config)
    print("‚úÖ Callbacks configured")
    
    # Start training
    print("\nüéì Starting training...")
    print(f"Total timesteps: {config['algorithm']['total_timesteps']}")
    print("This is where the magic happens - the agent will now:")
    print("  1. Take actions in the traffic environment")
    print("  2. Observe the results (rewards)")
    print("  3. Update its policy to get better rewards")
    print("  4. Repeat thousands of times!")
    print()
    
    start_time = datetime.now()
    
    # THE MAIN TRAINING LOOP
    model.learn(
        total_timesteps=config['algorithm']['total_timesteps'],
        callback=callbacks,
        progress_bar=False  # Disable progress bar to avoid display issues
    )
    
    end_time = datetime.now()
    training_duration = end_time - start_time
    
    # Save final model
    model_filename = f"models/ppo_traffic_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    model.save(model_filename)
    
    print("\nüéâ Training completed!")
    print(f"‚è±Ô∏è  Training duration: {training_duration}")
    print(f"üíæ Final model saved as: {model_filename}")
    print(f"üìä Tensorboard logs: ./logs/tensorboard/")
    print(f"üîÑ Checkpoints saved in: models/checkpoints/")
    
    # Close environment
    env.close()
    
    return model, model_filename


if __name__ == "__main__":
    print("üö¶ Traffic Signal RL Training")
    print("This script will train an AI agent to control traffic lights!")
    print()
    
    # Check if SUMO is available
    try:
        import traci
        print("‚úÖ SUMO/TraCI available")
    except ImportError:
        print("‚ùå SUMO/TraCI not found. Please install SUMO first.")
        sys.exit(1)
    
    # Check if PyTorch is using GPU
    if torch.cuda.is_available():
        print(f"üöÄ GPU available: {torch.cuda.get_device_name()}")
    else:
        print("üíª Using CPU for training")
    
    print()
    
    # Run training
    try:
        model, model_path = train_agent()
        print("\n‚úÖ Training completed successfully!")
        print(f"Your trained agent is ready at: {model_path}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc() 